{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the info of the restaurants that the Instagram pages suggest:\n",
    "\n",
    "Using a local clone of `llama3` and `langchain` I collect the following information from each post caption that I saved in the last [notebook](./caption_scraper.ipynb):\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"restaurant_name\": \"\",\n",
    "    \"address\": \"\",\n",
    "    \"instagram\": \"\",\n",
    "    \"famous_for\": \"\",\n",
    "    \"location_tag\": [],\n",
    "    \"cuisine\": []\n",
    "}\n",
    "```\n",
    "\n",
    "The information is extracted via a series of prompts in the following logical order:\n",
    "\n",
    "1. We loop over each post for each Instagram page and ask the model to determine if the caption is talking about a food or drinks place.\n",
    "2. If the caption is about a restaurant, we extract the restaurant name, address, instagram handle, a one line description of what they are known for and the relevant location tags that specify which neighborhood they are in LA.\n",
    "3. If the restaurant address is not available, then we leave it empty.\n",
    "4. We then ask the model to determine the location of the restaurant based on the restaurant name and location tag.\n",
    "5. We then ask the model to fix the JSON object extracted from the previous prompt. (**for some reason the model does not always return a valid JSON object**)\n",
    "6. Finally, we ask the model to determine the type of cuisine the restaurant serves based on the caption.\n",
    "7. We save the extracted information in a JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def response_to_json(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract the JSON part from the LLM response.\n",
    "\n",
    "    Parameters:\n",
    "    - response: str: The response from the LLM.\n",
    "\n",
    "    Returns:\n",
    "    - Optional[Dict[str, Any]]: The extracted JSON part, if found; otherwise, None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Regular expression to match JSON object\n",
    "        json_match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            print(\"No JSON object found in the response.\")\n",
    "            return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "def response_to_json_llm(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fix the LLM response to ensure it returns a proper JSON dictionary object.\n",
    "\n",
    "    Parameters:\n",
    "    - response: str: The response from the LLM.\n",
    "\n",
    "    Returns:\n",
    "    - Optional[Dict[str, Any]]: The corrected JSON dictionary, if possible; otherwise, None.\n",
    "    \"\"\"\n",
    "    global fix_json_llm_chain\n",
    "    fixed_response = fix_json_llm_chain.run({\"response\": response})\n",
    "    # print(f\"Fixed Response: {fixed_response}\")\n",
    "    try:\n",
    "        return json.loads(fixed_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # print(f\"Error decoding fixed JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_captions_from_file(filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load captions from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: str: The name of the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of captions.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_caption_with_llm(caption: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use the LLM chain to analyze a caption and determine if it's about a restaurant,\n",
    "    and extract the name and address if available.\n",
    "\n",
    "    Parameters:\n",
    "    - caption: str: The caption to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: The analysis result containing whether it's about a restaurant,\n",
    "                      the restaurant name, and the address (if available).\n",
    "    \"\"\"\n",
    "    global analyze_llm_chain\n",
    "    response = analyze_llm_chain.run({\"caption\": caption})\n",
    "    # print(f\"LLM Response: {response}\")\n",
    "    analysis = response_to_json(response)\n",
    "    \n",
    "    if analysis is None:\n",
    "        analysis = response_to_json_llm(response)\n",
    "\n",
    "    # print(f\"Scraped Response: {analysis}\")\n",
    "    return analysis\n",
    "\n",
    "\n",
    "def guess_address_llm(restaurant_name: str, location_tag: list) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Guess the address of a restaurant based on its name using the Ollama model.\n",
    "\n",
    "    Parameters:\n",
    "    - restaurant_name: str: The name of the restaurant.\n",
    "    - location_tags: list: The location tags extracted from the caption.\n",
    "\n",
    "    Returns:\n",
    "    - Optional[str]: The guessed address, if found; otherwise, None.\n",
    "    \"\"\"\n",
    "    global address_llm_chain\n",
    "    response = address_llm_chain.run({\"restaurant_name\": restaurant_name, \"location_tag\": location_tag})\n",
    "    return response.strip()\n",
    "\n",
    "def get_cuisine_llm(restaurant_name: str, caption: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get the cuisine of a restaurant based on its name and the caption.\n",
    "    \n",
    "    Parameters:\n",
    "    - restaurant_name: str: The name of the restaurant.\n",
    "    - caption: str: The caption containing additional information.\n",
    "    \"\"\"\n",
    "    global cuisine_llm_chain\n",
    "    response = cuisine_llm_chain.run({\"restaurant_name\": restaurant_name, \"caption\": caption})\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def process_single_caption(caption: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single caption to analyze if it is about a restaurant and extract relevant information.\n",
    "\n",
    "    Parameters:\n",
    "    - caption: str: The caption to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: The processed result containing analysis details for the caption.\n",
    "    \"\"\"\n",
    "    analysis = analyze_caption_with_llm(caption)\n",
    "    \n",
    "    if analysis.get('is_about_restaurant') and analysis.get('restaurant_name'):\n",
    "        if not analysis.get('address'):\n",
    "            analysis['address'] = guess_address_llm(analysis.get('restaurant_name'), analysis.get('location_tag', []))\n",
    "        analysis['cuisine'] = get_cuisine_llm(analysis.get('restaurant_name'), caption)\n",
    "    analysis.pop('is_about_restaurant', None)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def extract_restaurant_info(captions: List[str], insta_account: str):\n",
    "    \"\"\"\n",
    "    Process the captions to analyze if they are about a restaurant and extract relevant information to save it to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - captions: List[str]: The list of captions to analyze.\n",
    "    - insta_account: str: The Instagram account name.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    filename = f\"results/{insta_account.replace('posts/', 'recs_by_')}\"\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_single_caption, caption) for caption in captions]\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write('[')\n",
    "            \n",
    "            first_result = True\n",
    "            for future in tqdm(as_completed(futures), total=len(captions)):\n",
    "                result = future.result()\n",
    "                \n",
    "                if not result:\n",
    "                    continue  # Skip None or empty results\n",
    "                \n",
    "                if not first_result:\n",
    "                    f.write(',\\n')\n",
    "                json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "                \n",
    "                # Explicitly delete the result and the future to free memory\n",
    "                del result\n",
    "                del future\n",
    "                first_result = False\n",
    "\n",
    "                # Run garbage collector to free up memory\n",
    "                gc.collect()\n",
    "            \n",
    "            f.write(']')\n",
    "\n",
    "            \n",
    "def save_analysis_to_file(analysis: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the analysis results to a file in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "    - analysis: List[Dict[str, Any]]: The analysis results to save.\n",
    "    - filename: str: The name of the file to save the results to.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ollama model\n",
    "llm = Ollama(model=\"llama3:8b-instruct-q5_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"caption\"],\n",
    "    template=\"\"\"\n",
    "    Analyze the following caption and determine if it's talking about a food or drinks place!\n",
    "    If it is, extract the restaurant name, address, instagram handle and the relevant location tags that SPECIFY which neighborhood they are in LA (don't include vague tags like losangeles, lafoodie, california, etc). \n",
    "    Also, include a one line description of what they are known for.\n",
    "    If the resturant address is not available, then leave it empty.\n",
    "    Return the result in the following JSON format and format ONLY! Say absolutely nothing else! Ensure that both the JSON braces are present.\n",
    "    {{\n",
    "        \"is_about_restaurant\": <true/false>,\n",
    "        \"restaurant_name\": \"<restaurant_name>\",\n",
    "        \"address\": \"<address>\",\n",
    "        \"instagram\": \"<instagram_handle>\",\n",
    "        \"famous_for\": \"<famous_for>\", \n",
    "        \"location_tag\": \"<location_tag>\" // list of location tags \n",
    "    }}\n",
    "    Caption: {caption}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define the prompt template for guessing the address\n",
    "address_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\",\"location_tag\"],\n",
    "    template=\"\"\"\n",
    "    You are a YellowPages book operator and you know all the locations of resturants in '{location_tag}'. Where is the restaurant '{restaurant_name}' located? \n",
    "    Respond only and ONLY with the full address of the restaurant. Absolutely, do not say anything else or you may lose your job!\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the prompt template for fixing the JSON response\n",
    "fix_json_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"response\",],\n",
    "    template=\"\"\"\n",
    "    The following text contains a JSON object, but it might have some formatting issues or additional text. Extract and correct the JSON object so that it is a valid JSON dictionary:\n",
    "    Response: {response}\n",
    "    Corrected JSON:\n",
    "    Ensure that both the JSON braces are present. Say absolutely nothing else! Your response should only contain the corrected JSON object ONLY!!!\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the prompt for assigning the cuisine type\n",
    "cuisine_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\",\"caption\"],\n",
    "    template=\"\"\"\n",
    "    You are a food critic and you know all the cuisines of the world. Based on this article - `{caption}`, what type of cuisine does the restaurant '{restaurant_name}' serve?\n",
    "    Respond only and ONLY with the type of cuisine in a list format (e.g. [\"Chinese\"] or [\"Chinese\", \"Cantonese\"] are valid responses). Absolutely, do not say anything else or you may lose your job!\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM chains\n",
    "analyze_llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "address_llm_chain = LLMChain(llm=llm, prompt=address_prompt_template)\n",
    "fix_json_llm_chain = LLMChain(llm=llm, prompt=fix_json_prompt_template)\n",
    "cuisine_llm_chain = LLMChain(llm=llm, prompt=cuisine_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_names = [\"dinertheory\",\"infatuation_la\",\"kcrwgoodfood\",\"la.ethnic.eats\",\"lacoffeelist\",\"ricklox\",\"thelacountdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_accounts = [f\"posts/{i}.json\" for i in account_names]\n",
    "for insta_account in insta_accounts:\n",
    "      captions = load_captions_from_file(insta_account)\n",
    "      extract_restaurant_info(captions, insta_account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "- [ ] continue from entry 526 from infatuation\n",
    "\n",
    "- [ ] continue from entry 111 from ricklox \n",
    "\n",
    "- [ ] process kcrwgoodfood\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
